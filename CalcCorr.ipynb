{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This  just a testCase for Cal Corr for a given subject i from timeseries input data\n",
    "\n",
    "The goal of this NoteBook file is to <br>\n",
    "1. Create timeseries Feature tables (.csv) for each subject, labeled by actual area name\n",
    "2. Calculate Correlation Matrix FC\n",
    "3. Calculate also distance matrix (not used in the paper)\n",
    "4. Save all Corrs and Dist matrices as tables with actual labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'E:\\AbideI\\ProcessedData\\\\ex_file.csv'\n",
    "ex_file_path = 'E:\\\\AbideI\\\\Outputs\\\\cpac\\\\nofilt_noglobal\\\\rois_aal\\\\Caltech_0051456_rois_aal.1D'\n",
    "# For atlas conversion to csv (from atlasConverter)\n",
    "aal_atlas_path = 'E:\\\\AbideI\\\\Atlases\\\\aal_labels.csv'\n",
    "tt_atlas_path = 'E:\\\\AbideI\\\\Atlases\\\\tt_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an input df, with names as in the standard downloaded labels csv, and convert column names\n",
    "# atlas_df: df of first column contains labels, second contains brain regions\n",
    "# atlas_df contains NO non-label rows. atlas_df contains prefix # value for each label\n",
    "def parse_brain_regions(df, atlas_df= pd.read_csv(aal_atlas_path, skiprows=2, header=None)):\n",
    "    if atlas_df is parse_brain_regions.__defaults__[0]:\n",
    "        # For aal and tt atlas, if df is not passed, do this first. \n",
    "        atlas_df[0] = atlas_df[0].apply(lambda x: '#'+ str(x))\n",
    "        print('note: aal atlas was used, hashed labels')\n",
    "    out_df = df.copy()\n",
    "    out_df.rename(columns=dict(zip(atlas_df[0], atlas_df[1])), inplace=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReadAtalsFiles, Add preceeding hash first to atlas as in the label names\n",
    "aal_atlas_df = pd.read_csv(aal_atlas_path, skiprows=2, header=None)\n",
    "aal_atlas_df[0] = aal_atlas_df[0].apply(lambda x: '#'+ str(x))\n",
    "tt_atlas_df = pd.read_csv(tt_atlas_path, skiprows=2, header=None) # First two rows contains non-atlas data. Check\n",
    "tt_atlas_df[0] = tt_atlas_df[0].apply(lambda x: '#'+ str(x)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DriverCodeExample: \n",
    "input_df_aal = pd.read_csv('E:\\\\AbideI\\\\Outputs\\\\cpac\\\\nofilt_noglobal\\\\rois_aal\\\\Caltech_0051456_rois_aal.1D', header=0, sep='\\t')\n",
    "input_df_tt = pd.read_csv('E:\\\\AbideI\\\\Outputs\\\\cpac\\\\nofilt_noglobal\\\\rois_tt\\\\Caltech_0051456_rois_tt.1D', header=0, sep='\\t')\n",
    "parse_brain_regions(input_df_aal,aal_atlas_df).to_csv('E:\\\\AbideI\\\\ProcessedData\\\\ex_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDataMats(corr_dict, diff_dict, feat_dict, nameslabelsdf):\n",
    "    nameslabelsdf.set_index('subjectkey', inplace=True)\n",
    "\n",
    "    # Create columns for correlation and SSD mats\n",
    "    columns_diff_corr = []\n",
    "    for k in range(nAreas):\n",
    "        for j in range(k+1, nAreas):\n",
    "            columns_diff_corr.append(f'{k}_{j}')\n",
    "    columns_diff_corr.append('labels')\n",
    "    corr_df = pd.DataFrame(None, index=nameslabelsdf.index, columns=columns_diff_corr)\n",
    "    diff_df = pd.DataFrame(None, index=nameslabelsdf.index, columns=columns_diff_corr)\n",
    "\n",
    "    # Create columns for feat mat\n",
    "    columns_feat = []\n",
    "    for i in range(nAreas):\n",
    "        for j in feats_names:\n",
    "            columns_feat.append(f'{i}_{j}')\n",
    "    columns_feat.append('labels')\n",
    "    feat_df = pd.DataFrame(None, index=nameslabelsdf.index, columns=columns_feat)\n",
    "\n",
    "    # Start filling correlation and diff mats together\n",
    "    # for key in nameslabelsdf.index:\n",
    "    for key in corr_dict.keys():\n",
    "        corr_mat = corr_dict[key]\n",
    "        diff_mat = diff_dict[key]\n",
    "        feat_mat = feat_dict[key]\n",
    "        label = nameslabelsdf.loc[key, 'Dx']\n",
    "        for ind in columns_diff_corr:\n",
    "            if ind == 'labels':\n",
    "                continue\n",
    "            a1, a2 = ind.split('_')\n",
    "            a1 = int(a1); a2=int(a2)\n",
    "            corr_df.loc[key,ind] = corr_mat[a1, a2]\n",
    "            diff_df.loc[key,ind] = diff_mat[a1, a2]\n",
    "        corr_df.loc[key,'labels'] = label\n",
    "        diff_df.loc[key,'labels'] = label\n",
    "\n",
    "        feat_df.loc[key, :-1] = feat_mat.ravel()\n",
    "        feat_df.loc[key,'labels'] = label\n",
    "\n",
    "    return corr_df, diff_df, feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating correlation matrix for a single file, written as csv\n",
    "def CreateCorr(filepath):\n",
    "    data_df = pd.read_csv(filepath)\n",
    "    nAreas = data_df.shape[1]\n",
    "    temp_corr = np.zeros((nAreas, nAreas))\n",
    "    temp_diff = np.zeros_like(temp_corr)\n",
    "    col_nam=[]\n",
    "    for i in range(nAreas):\n",
    "        temp_corr[i,i] = 1\n",
    "        temp_diff[i,i] = 0\n",
    "        for j in range(i+1, nAreas):\n",
    "            temp_corr[i,j] = np.corrcoef(data_df.iloc[:,i], data_df.iloc[:,j])[0,1] # It returns the covar matrix of 2x2\n",
    "            temp_corr[j,i] = temp_corr[i,j]\n",
    "            temp_diff[i,j] = np.sqrt(np.sum(np.square(data_df.iloc[:,i]-data_df.iloc[:,j])))\n",
    "            temp_diff[j,i] = temp_diff[i,j] \n",
    "#             col_nam.append( '__'.join([df.columns[j] ,df.columns[i]]) )\n",
    "    temp_corr = np.nan_to_num(temp_corr)\n",
    "    temp_diff = np.nan_to_num(temp_diff)\n",
    "    return temp_corr, temp_diff            # col_nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_names(col_names):\n",
    "    col_nam = []\n",
    "    n_areas = len(col_names)\n",
    "    for i in range(n_areas):\n",
    "        for j in range(i+1, n_areas):\n",
    "            col_nam.append( '__'.join([col_names[i].replace(' ', '') ,col_names[j].replace(' ', '')] ) )\n",
    "    return col_nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feat_matrices(FolderPath, atlas_df, output_path = 'E:\\\\AbideI\\\\ProcessedData\\\\nofilt_noglobal_rois_aal\\\\feat\\\\', atl_name='aal'):\n",
    "    subjs_fldrs = [os.path.join(FolderPath, f) for f in os.listdir(FolderPath)]\n",
    "    for filepath in subjs_fldrs:\n",
    "        subj = filepath.split('\\\\')[-1].split('.')[0]\n",
    "        input_df_aal = pd.read_csv(filepath, header=0, sep='\\t')\n",
    "        parse_brain_regions(input_df_aal,atlas_df).to_csv(output_path + subj+ '.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create_feat_matrices('E:\\\\AbideI\\\\Outputs\\\\cpac\\\\nofilt_noglobal\\\\rois_aal\\\\', aal_atlas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_matrices(FolderPath, col_names, output_path = 'E:\\\\AbideI\\\\ProcessedData\\\\nofilt_noglobal_rois_aal\\\\corr\\\\'):\n",
    "    corr_col_names = create_corr_names(col_names)\n",
    "    subjects_corr_df = pd.DataFrame(columns = corr_col_names)\n",
    "    subjects_dist_df = pd.DataFrame(columns = corr_col_names)\n",
    "    subjs_fldrs = [os.path.join(FolderPath, f) for f in os.listdir(FolderPath)]\n",
    "    for filepath in subjs_fldrs:\n",
    "        subj = filepath.split('\\\\')[-1].split('.')[0]\n",
    "#         if os.path.exists(output_path + subj+ '_corr.csv'): #unhash if needed\n",
    "#             print(f'Subj %s already exists, continuing') \n",
    "#             continue\n",
    "        sub_name = ('_').join(subj.split('_')[:2])\n",
    "        print('working on Subj: ', subj)\n",
    "        corr, dist = CreateCorr(filepath)\n",
    "        # Save each file independently:\n",
    "        pd.DataFrame(data=corr, columns = col_names).to_csv(output_path + subj+ '_corr.csv',index = False)\n",
    "        pd.DataFrame(data=dist, columns = col_names).to_csv(output_path + subj+ '_dist.csv',index = False)\n",
    "        corr_vals = corr[np.triu_indices_from(corr, k=1)]\n",
    "        dist_vals = dist[np.triu_indices_from(dist, k=1)]\n",
    "        subjects_corr_df.loc[subj,:] = corr_vals.reshape((1,-1))\n",
    "        subjects_dist_df.loc[subj,:] = dist_vals.reshape((1,-1))\n",
    "#         if subjects_dist_df.shape[0]==5:#debug only\n",
    "#             subjects_corr_df\n",
    "    subjects_corr_df.to_csv(output_path + '_CORR.csv',index = True)\n",
    "    subjects_dist_df.to_csv(output_path + '_DIST.csv',index = True)\n",
    "    return subjects_corr_df, subjects_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'E:\\\\AbideI\\\\Outputs\\\\cpac\\\\'\n",
    "sub_pth_tt = 'Caltech_0051456_rois_tt.1D'\n",
    "sub_pth_aal = 'Caltech_0051456_rois_aal.1D'\n",
    "out_pth = 'E:\\\\AbideI\\\\ProcessedData\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 1: nonfilt nonglob aal atlas:\n",
    "filt_pth = 'nofilt_noglobal\\\\'\n",
    "atl_pth = 'rois_aal\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_aal), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, aal_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), aal_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df, diff_df = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 2: nonfilt nonglob tt atlas:\n",
    "filt_pth = 'nofilt_noglobal\\\\'\n",
    "atl_pth = 'rois_tt\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_tt), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, tt_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), tt_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df2, diff_df2 = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#DebugCell\n",
    "filt_pth = 'nofilt_noglobal\\\\'\n",
    "atl_pth = 'rois_tt\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth), header=0, sep='\\t')\n",
    "col_df= parse_brain_regions(ex_df, tt_atlas_df)\n",
    "colnames2 = create_corr_names(col_df.columns)\n",
    "print(col_df.shape)\n",
    "print(len(colnames2))\n",
    "col_df.head()\n",
    "# parse_brain_regions(ex_df, tt_atlas_df).columns.shape\n",
    "# corr_df2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#DebugCell\n",
    "97*96/2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#DebugCell\n",
    "subjects_corr_df = pd.DataFrame(columns = corr_col_names)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#DebugCell\n",
    "corr_df2.head()\n",
    "subj = filepath.split('\\\\')[-1].split('.')[0]\n",
    "#         if os.path.exists(output_path + subj+ '_corr.csv'): #unhash if needed\n",
    "#             print(f'Subj %s already exists, continuing') \n",
    "#             continue\n",
    "sub_name = ('_').join(subj.split('_')[:2])\n",
    "ex_df = pd.read_csv(filepath, header=0)\n",
    "colnames2 = create_corr_names(ex_df.columns)\n",
    "print('working on Subj: ', subj)\n",
    "corr_, dist_ = CreateCorr(filepath)\n",
    "# Save each file independently:\n",
    "co_df_ex = pd.DataFrame(data=corr_, columns = ex_df.columns)\n",
    "di_df_ex = pd.DataFrame(data=dist_, columns = ex_df.columns)\n",
    "corr_vals_ = corr_[np.triu_indices_from(corr_, k=1)]\n",
    "dist_vals_ = dist_[np.triu_indices_from(dist_, k=1)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr_vals_.reshape((1,-1)).shape\n",
    "subjects_corr_df_ex.loc[subj,:] = corr_vals_.reshape((1,-1))\n",
    "subjects_corr_df_ex"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subjects_corr_df_ex.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subjects_corr_df_ex = pd.DataFrame(columns = colnames2)\n",
    "print(subjects_corr_df_ex)\n",
    "subjects_corr_df_ex.append(pd.DataFrame(data = corr_vals_.reshape((1,-1)), index= [subj], columns= colnames2))\n",
    "subjects_corr_df_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 3: nofilt_global aal atlas:\n",
    "filt_pth = 'nofilt_global\\\\'\n",
    "atl_pth = 'rois_aal\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_aal), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, aal_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), aal_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df3, diff_df3 = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 4: nofilt_global tt atlas:\n",
    "filt_pth = 'nofilt_global\\\\'\n",
    "atl_pth = 'rois_tt\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_tt), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, tt_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), tt_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df4, diff_df4 = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 5: filt_noglobal aal atlas:\n",
    "filt_pth = 'filt_noglobal\\\\'\n",
    "atl_pth = 'rois_aal\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_aal), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, aal_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), aal_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df5, diff_df5 = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 6: filt_noglobal tt atlas:\n",
    "filt_pth = 'filt_noglobal\\\\'\n",
    "atl_pth = 'rois_tt\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_tt), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, tt_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), tt_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df6, diff_df6 = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 7: filt_global aal atlas:\n",
    "filt_pth = 'filt_global\\\\'\n",
    "atl_pth = 'rois_aal\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_aal), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, aal_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), aal_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df7, diff_df7 = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Working on 8: filt_global tt atlas:\n",
    "filt_pth = 'filt_global\\\\'\n",
    "atl_pth = 'rois_tt\\\\'\n",
    "mrgd_pth = filt_pth.strip('\\\\') + '_' + atl_pth\n",
    "ex_df = pd.read_csv((main_path+filt_pth+atl_pth+sub_pth_tt), header=0, sep='\\t')\n",
    "colnames = parse_brain_regions(ex_df, tt_atlas_df).columns\n",
    "create_feat_matrices((main_path+filt_pth+atl_pth), tt_atlas_df, output_path = out_pth+mrgd_pth+'feat\\\\')\n",
    "corr_df8, diff_df8 = create_corr_matrices(out_pth+mrgd_pth+'feat\\\\', colnames, output_path =out_pth+mrgd_pth+'corr\\\\' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algortithm<br>\n",
    "I) Read Each subject values, and convert area names<br>\n",
    "II) Calc Corr, Diff, and other useful metrics that may encode the features. [signal representatives] <br>\n",
    "III) Then, For each Feature type\n",
    "1) Create column names, then create empty df <br>\n",
    "2) for each index (Subject name) add to each subject his feature values. <br>\n",
    "3) Save two csv files, indexed by subject names.<br>\n",
    "- Subject name is [:-2], where some has middle identifiers, better remove underscore for consistent naming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
